# Shayan Nicolas Hollet (Matricule: 20146766)
# Byungsuk Min (Matricule: 20234231)

#!/usr/bin/env python
# coding: utf-8


# Import the libraries

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt


class PracticalHomework2:

    def __init__(self):
        """
        No input is required.
        """
        pass


    
    def generate_data(self, seed):
        """
        [DO NOT CHANGE THIS METHOD]
        Generates synthetic dataset with Gaussian mixtures and saves it as CSV files.

        Input:
        - No input required for this method. It generates synthetic data based on pre-defined parameters.

        Process:
        - Creates a dataset with 3 classes where each class is represented by samples drawn from a multivariate normal distribution.
        - The data is shuffled and split into training (60%), validation (20%), and test (20%) sets.
        - The data and labels for each split are saved as CSV files:
          - 'train_features.csv', 'train_labels.csv'
          - 'val_features.csv', 'val_labels.csv'
          - 'test_features.csv', 'test_labels.csv'

        Output:
        - No direct output is returned by this method. The generated data is saved as CSV files.
        """
        np.random.seed(seed)
        
        # Parameters for Gaussian mixtures
        means = [np.random.rand(10) * 2 - 1 for _ in range(3)]
        covs = [np.eye(10) * 0.5 for _ in range(3)]
        
        # Generate data for each class
        X_class0 = np.random.multivariate_normal(means[0], covs[0], 500)
        X_class1 = np.random.multivariate_normal(means[1], covs[1], 500)
        X_class2 = np.random.multivariate_normal(means[2], covs[2], 500)
        
        # Create labels
        y_class0 = np.zeros(500)
        y_class1 = np.ones(500)
        y_class2 = np.ones(500) * 2
        
        # Concatenate the data
        X = np.vstack([X_class0, X_class1, X_class2])
        y = np.hstack([y_class0, y_class1, y_class2])
        
        # Shuffle the data
        indices = np.arange(X.shape[0])
        np.random.shuffle(indices)
        X = X[indices]
        y = y[indices]
        
        # Split the data into training, validation, and test sets
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
        
        # Save to CSV
        pd.DataFrame(X_train).to_csv('train_features.csv', index=False)
        pd.DataFrame(y_train).to_csv('train_labels.csv', index=False)
        pd.DataFrame(X_val).to_csv('val_features.csv', index=False)
        pd.DataFrame(y_val).to_csv('val_labels.csv', index=False)
        pd.DataFrame(X_test).to_csv('test_features.csv', index=False)
        pd.DataFrame(y_test).to_csv('test_labels.csv', index=False)
    
    def load_and_preprocess_data(self):
        """
        [DO NOT CHANGE THIS METHOD]
        Loads data from CSV files and standardizes the features using StandardScaler.

        Input:
        - The method expects CSV files generated by the 'generate_data' method:
          - 'train_features.csv', 'train_labels.csv'
          - 'val_features.csv', 'val_labels.csv'
          - 'test_features.csv', 'test_labels.csv'

        Process:
        - Reads the CSV files containing the training, validation, and test features and labels.
        - Standardizes the features (i.e., scales them to have zero mean and unit variance) using `StandardScaler`.
        - Applies the scaling transformation to the training, validation, and test sets.

        Output:
        - Returns six NumPy arrays:
          1. `X_train`: The standardized training feature set.
          2. `y_train`: The labels for the training set.
          3. `X_val`: The standardized validation feature set.
          4. `y_val`: The labels for the validation set.
          5. `X_test`: The standardized test feature set.
          6. `y_test`: The labels for the test set.
        """
        # Load the data
        X_train = pd.read_csv('train_features.csv').values
        y_train = pd.read_csv('train_labels.csv').values.flatten()
        X_val = pd.read_csv('val_features.csv').values
        y_val = pd.read_csv('val_labels.csv').values.flatten()
        X_test = pd.read_csv('test_features.csv').values
        y_test = pd.read_csv('test_labels.csv').values.flatten()

        # Standardize the features
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_val = scaler.transform(X_val)
        X_test = scaler.transform(X_test)

        return X_train, y_train, X_val, y_val, X_test, y_test

    def make_one_versus_all_labels(self, y, num_classes):
        """
        Converts the given labels into a one-vs-all format for multi-class classification.

        Input:
        - y: Array of shape (n_samples,) containing the original class labels.
        - num_classes: Integer representing the total number of classes.

        Process:
        - Creates a binary label array where each row corresponds to one sample, and the columns
          represent one-vs-all labels for each class.

        Output:
        - Returns a label array of shape (n_samples, num_classes) with -1 for non-class columns
          and 1 for the true class column.
        """

        # Convert y to a NumPy array of integers if it's not already
        y = np.asarray(y, dtype=int)

        # Validate that y is a 1-dimensional array with valid class labels
        if y.ndim != 1:
            raise ValueError("Labels y must be a one-dimensional array.")
        if not np.all((0 <= y) & (y < num_classes)):
            raise ValueError(f"All labels must be between 0 and {num_classes - 1} inclusive.")

        # Create the label matrix Y using broadcasting, aligning with the definition of t_i^j
        # Y[i, j] = 1 if y[i] == j, else -1
        Y = np.where(np.equal.outer(y, np.arange(num_classes)), 1, -1)

        return Y

    def compute_loss(self, X, y, w, C):
        """
        Computes the logistic loss for multi-class classification.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
        - w: Weight matrix of shape (n_features, n_classes).
        - C: Regularization parameter (float).

        Process:
        - Computes the loss using a logistic regression formulation.
        - Adds L2 regularization term based on the weight matrix.

        Output:
        - Returns the scalar loss value (float).
        """

        # Number of samples in the mini-batch
        n_samples = X.shape[0]

        # Compute the scores s_i^j = X_i · w^j for all examples and classes
        scores = X @ w  # Shape: (n_samples, num_classes)

        # Compute z_i^j = 2 - t_i^j * s_i^j
        z = 2 - y * scores  # Shape: (n_samples, num_classes)

        # Compute the hinge loss component: (max(0, z_i^j))^2
        hinge_losses = np.maximum(0, z) ** 2  # Shape: (n_samples, num_classes)

        # Compute the mean hinge loss over the mini-batch
        loss_hinge = np.sum(hinge_losses) / n_samples

        # Compute the L2 regularization term: (C / 2) * ||w||^2
        loss_reg = (C / 2) * np.sum(w ** 2)

        # Total loss is the sum of hinge loss and regularization loss
        loss = loss_hinge + loss_reg

        return loss

    def compute_gradient(self, X, y, w, C):
        """
        Computes the gradient of the logistic loss for multi-class classification.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
        - w: Weight matrix of shape (n_features, n_classes).
        - C: Regularization parameter (float).

        Process:
        - Computes the gradient of the logistic loss with respect to the weights.
        - Adds the gradient of the L2 regularization term.

        Output:
        - Returns the gradient matrix of shape (n_features, n_classes).
        """
        n_samples = X.shape[0]

        # Compute the scores s_i^j = X_i · w^j for all examples and classes
        scores = X @ w  # Shape: (n_samples, num_classes)

        # Compute z_i^j = 2 - t_i^j * s_i^j
        z = 2 - y * scores  # Shape: (n_samples, num_classes)

        # Compute the indicator function I_{ z_i^j > 0 }
        indicator = (z > 0)  # Shape: (n_samples, num_classes)

        # Compute the gradient of the hinge loss
        # temp = (2 - t_i^j s_i^j) * t_i^j * indicator = z_i^j * t_i^j * indicator
        temp = z * y * indicator  # Shape: (n_samples, num_classes)

        # Compute the gradient of the hinge loss component
        grad_hinge = (-2 / n_samples) * X.T @ temp  # Shape: (n_features, num_classes)

        # Compute the gradient of the regularization term
        grad_reg = C * w  # Shape: (n_features, num_classes)

        # Total gradient is the sum of the hinge loss gradient and the regularization gradient
        grad = grad_hinge + grad_reg

        return grad

    def infer(self, X, w):
        """
        Predicts the class labels for a given feature matrix.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - w: Weight matrix of shape (n_features, n_classes).

        Process:
        - Computes the predicted class probabilities for each sample.
        - Assigns the class with the highest probability as the predicted class label.

        Output:
        - Returns an array of predicted class labels of shape (n_samples,).
        """
        
        # Compute the scores for each class
        scores = X @ w  # Shape: (n_samples, n_classes)

        # Determine the predicted class indices by finding the argmax of the scores
        predicted_classes = np.argmax(scores, axis=1)  # Shape: (n_samples,)

        # Return the predicted class indices (integers between 0 and num_classes-1)
        return predicted_classes

    def fit(
        self, X_train, y_train, X_val, y_val,
        num_classes, epochs,
        learning_rate, C, batch_size
    ):
        """
        [DO NOT CHANGE THIS METHOD]
        Fits a multi-class logistic regression model using mini-batch gradient descent.

        Input:
        - X_train: Feature matrix of shape (n_train, n_features) for training.
        - y_train: Label matrix of shape (n_train, n_classes) for training.
        - X_val: Feature matrix of shape (n_val, n_features) for validation.
        - y_val: Label matrix of shape (n_val, n_classes) for validation.
        - num_classes: Integer representing the total number of classes.
        - epochs: Integer specifying the number of training epochs.
        - learning_rate: Float value specifying the learning rate for optimization.
        - C: Regularization parameter (float).
        - batch_size: Integer specifying the mini-batch size for training.

        Process:
        - Initializes the weight matrix with zeros.
        - Iterates over the training data for the specified number of epochs.
        - Splits the training data into mini-batches and performs gradient descent.
        - Computes the training and validation losses at the end of each epoch.

        Output:
        - Returns the weight matrix after training.
        """
        num_features = X_train.shape[1]
        w = np.zeros((num_features, num_classes))
        
        # Dictionaries to store the metrics
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
        
        for epoch in range(epochs):
            indices = np.arange(X_train.shape[0])
            np.random.shuffle(indices)
            
            # Mini-batch training
            for i in range(0, X_train.shape[0], batch_size):
                batch_indices = indices[i:i+batch_size]
                X_batch = X_train[batch_indices]
                y_batch = self.make_one_versus_all_labels(y_train[batch_indices], num_classes)
                
                # Compute gradients and update weights
                grad = self.compute_gradient(X_batch, y_batch, w, C)
                w -= learning_rate * grad
            
            # Compute training and validation loss and accuracy
            train_loss = self.compute_loss(X_train, self.make_one_versus_all_labels(y_train, num_classes), w, C)
            val_loss = self.compute_loss(X_val, self.make_one_versus_all_labels(y_val, num_classes), w, C)
            train_acc = self.compute_accuracy(X_train, y_train, w)
            val_acc = self.compute_accuracy(X_val, y_val, w)
        
            # Store the metrics
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)
            
            print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}")
        
        return w, history  # Return weights and history
    
    def compute_accuracy(self, X, y, w):
        """
        Computes the accuracy of predictions using the given weight matrix.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - y: True label vector of shape (n_samples,).
        - w: Weight matrix of shape (n_features, n_classes).

        Process:
        - Predicts the labels for the given feature matrix using the weights.
        - Compares predicted labels with true labels to compute accuracy.

        Output:
        - Returns the accuracy score (float) as a percentage of correct predictions.
        """

        # Predict the labels
        predicted_classes = self.infer(X, w) # Shape: (n_samples,)

        # Compute the proportion of correct predictions
        accuracy = np.mean(predicted_classes == y) 

        return accuracy

def main():
    # Instantiate the class
    phw2 = PracticalHomework2()

    # Generate data (if not already generated)
    # Uncomment the following line if you haven't generated the data yet
    # phw2.generate_data(seed=42)

    # Load and preprocess data
    X_train, y_train, X_val, y_val, X_test, y_test = phw2.load_and_preprocess_data()

    num_classes = 3
    epochs = 200
    learning_rate = 0.0001
    batch_size = 100

    Cs = [1, 5, 10]  # Different values of the regularization parameter C

    # Dictionary to store history for each value of C
    histories = {}

    for C in Cs:
        print(f"\nTraining with C = {C}")
        
        # Train the model and get weights and history
        w, history = phw2.fit(X_train, y_train, X_val, y_val, num_classes, epochs, learning_rate, C, batch_size)

        # Store the history
        histories[C] = history

    def plot_graphs(histories, epochs, Cs):
        epochs_range = range(1, epochs+1)

        # Plot training loss
        plt.figure(figsize=(10, 6))
        for C in Cs:
            plt.plot(epochs_range, histories[C]['train_loss'], label=f'C={C}')
        plt.title('Training Loss vs Epochs')
        plt.xlabel('Epochs')
        plt.ylabel('Training Loss')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Plot validation loss
        plt.figure(figsize=(10, 6))
        for C in Cs:
            plt.plot(epochs_range, histories[C]['val_loss'], label=f'C={C}')
        plt.title('Validation Loss vs Epochs')
        plt.xlabel('Epochs')
        plt.ylabel('Validation Loss')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Plot training accuracy
        plt.figure(figsize=(10, 6))
        for C in Cs:
            plt.plot(epochs_range, histories[C]['train_acc'], label=f'C={C}')
        plt.title('Training Accuracy vs Epochs')
        plt.xlabel('Epochs')
        plt.ylabel('Training Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Plot validation accuracy
        plt.figure(figsize=(10, 6))
        for C in Cs:
            plt.plot(epochs_range, histories[C]['val_acc'], label=f'C={C}')
        plt.title('Validation Accuracy vs Epochs')
        plt.xlabel('Epochs')
        plt.ylabel('Validation Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

    plot_graphs(histories, epochs, Cs)

if __name__ == '__main__':
    main()
